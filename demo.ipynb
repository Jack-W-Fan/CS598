{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "GIT Repo: https://github.com/JEF1056/CS598\n",
    "\n",
    "This project is based on the paper Dialogue-Contextualized Re-ranking for Medical History-Taking by Jian Zhu, Ilya Valmianski, and Antha Kannan. The paper itself focuses on the problem of on improving AI-driven medical history-taking, a crucial aspect of patient care and diagnosis. The paper proposes a two-stage reranking approach that involves using  an expert system to retrieve a list of candidate questions and then employing a machine-learned re-ranker to prioritize the most relevant question to ask. The paper also conducts experiments using real doctor-patient dialogue data collected from a medical service platform, Curai, which will be similar to our approach in data collection. The author hypothesizes that employing a two-stage re-ranking approach, incorporating a dialogue-contextualized model to prioritize relevant questions for medical history-taking, can effectively mitigate the training-inference gap encountered in AI-driven healthcare systems. The major contribution from this paper will be the main approach and reasoning/logic behind the decisions made while solving the intial problem. However, we will mainly use alternative models from those featured in the orginal study (ie. LongT5), in favor of employing more recent models trained on more general data. The code shown below will also be seperate from that of the paper will be designed and trained independently alongside llama-based models (utiliziing Tinyllama). The information provided in this paper serves as a baseline foundation for the code that we will now develop to take in a medical conversion and generate/reorder relevant questions/conversations, and sorting desired information.\n",
    "\n",
    "## Scope of reproducibility \n",
    "\n",
    "The dataset utilized in the paper is not publicly available for purchase or any other means of access. Therefore, we will employ a substitution approach by leveraging the MediQA dataset, which similarly encompasses medical conversational data and boasts a substantial amount of labeled information. This alternative dataset will serve as a suitable replacement for our analysis, allowing us to maintain the integrity and relevance of our study despite the unavailability of the original dataset. By leveraging a smaller scale model, we can accommodate more data per batch and utilize longer context lengths, thereby potentially enhancing the performance and robustness of our approach.\n",
    "\n",
    "\n",
    "# Install dependencies\n",
    "Before running, please install conda and run the following commands:\n",
    "\n",
    "This is a one-time setup, the `CS598` conda enviroment cna be activated at any time to activate these dependencies\n",
    "> I highly reccomend using conda and the following commands to easily enable CUDA support.\n",
    "\n",
    "```bash\n",
    "conda deactivate\n",
    "conda env remove -n CS598\n",
    "conda env create\n",
    "conda activate CS598\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -r requirements.txt\n",
    "```\n",
    "Select the new `CS598` conda kernel to begin\n",
    "\n",
    "## Environnent details\n",
    "**Python version:** Python 3.11\n",
    "\n",
    "**Packages used:** See `requirements.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 13:31:48.541109: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-13 13:31:48.573851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 13:31:49.268494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "\n",
    "# Training dependencies\n",
    "import torch\n",
    "from unsloth import FastLlamaModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.utils import logging\n",
    "from peft import PeftModel, LoftQConfig\n",
    "import multiprocessing\n",
    "from torch import backends\n",
    "from datasets import Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "backends.cuda.matmul.allow_tf32 = True\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "We focus on using the main dataset from [Mediqa 2023](https://github.com/abachaa/MTS-Dialog/tree/main/Main-Dataset), from which we will use a LLM [OpenHermes 2.5](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF) to extract candidate questions and create rankings for those questions.\n",
    "\n",
    "## High level processing steps\n",
    "Here is an example excerpt from the raw test set:\n",
    "```text\n",
    "Doctor: Good morning, ma'am. \n",
    "Patient: Good morning, doctor.\n",
    "Doctor: Before we begin today, I just need to confirm a few pieces of information I got from the nurse. \n",
    "Patient: Absolutely, no problem. \n",
    "Doctor: Great, so you're thirty six years old, correct? \n",
    "Patient: Yeah, that's right. \n",
    "Doctor: And you identify as Caucasian? \n",
    "Patient: Yes, doctor. \n",
    "Doctor: Thank you, young lady. So, what seems to be the problem today. \n",
    "Patient: Well, I've had pain in this right knee for a long time. \n",
    "Doctor: Have you been treated for this before? \n",
    "Patient: Yes, and I've been diagnosed with, um, chondromalacia. \n",
    "Doctor: How have you been treated so far? \n",
    "Patient: I've taken antiinflammatories, rested, changed my activities, all of that. \n",
    "Doctor: Has there been any improvement? \n",
    "Patient: No, none at all. \n",
    "Doctor: Have you discussed surgery with anyone before. \n",
    "Patient: No, nobody's said anything yet. \n",
    "Doctor: Well, I think you'd be a good candidate for an arthroscopy lateral release and tubercle transfer. \n",
    "Patient: What will the surgery do? Doctor: This will help take some stress off of the knee joint. It should help you feel a lot better. \n",
    "Patient: What are the risks of infection from the surgery, doctor? \n",
    "Doctor: Well, you'll be relieved to know that it's less than one percent. We use prophylactic antibiotics the entire time. \n",
    "Patient: Will I be asleep for this?\n",
    "Doctor: Yes, you won't feel a thing. \n",
    "Patient: Okay, yes. I agree, we should do the surgery.\n",
    "```\n",
    "\n",
    "1. To process this data, we select `N` indexes from the conversation where the doctor is the next speaker, where `N = (X - 2) // 5` and `X` is the length of the example conversation- in this case, 4.\n",
    "    - `N = (X - 5) // 5` was chosen because the first ~2-4 exchanges tend to be greetings and pleasantries and we want to reduce the numbers of new samples generated.\n",
    "2. For each of those indexes, we give our LLM expert a prompt to generate questions the doctor may want to ask, given the context of the exiting conversation.\n",
    "    - We can leverage GBNF grammar to force the LLM expert to produce a JSON-formatted array of strings.\n",
    "    - If the doctor's existing response is a question, we will include that in the array\n",
    "3. For each index, we ask the expert model to list in order the importance of the questions asked numerically. This is an array of integers.\n",
    "    - For example if the questions for index 6 are `[\"And you identify as Caucasian?\", \"You are Asian, correct?\", \"Have you ever visited other specialists for your inflamation issue?\"]` the ranking returned may be `[2, 1, 0]`\n",
    "4. Dump into examples. We have two distinct tasks, which are combined into a singe example. This improves computing efficiency and allows the task to be broken down into two steps. See below for an example:\n",
    "```text\n",
    "Context:\n",
    "Doctor: Good morning, ma'am. \n",
    "Patient: Good morning, doctor.\n",
    "Doctor: Before we begin today, I just need to confirm a few pieces of information I got from the nurse. \n",
    "Patient: Absolutely, no problem. \n",
    "Doctor: Great, so you're thirty six years old, correct? \n",
    "Patient: Yeah, that's right. \n",
    "\n",
    "Questions:\n",
    "[\"And you identify as Caucasian?\", \"You are Asian, correct?\", \"Have you ever visited other specialists for your inflamation issue?\"]\n",
    "\n",
    "Order:\n",
    "[2,1,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>section_header</th>\n",
       "      <th>section_text</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>The patient is a 76-year-old white female who ...</td>\n",
       "      <td>Doctor: What brings you back into the clinic t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>The patient is a 25-year-old right-handed Cauc...</td>\n",
       "      <td>Doctor: How're you feeling today?  \\r\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>This is a 22-year-old female, who presented to...</td>\n",
       "      <td>Doctor: Hello, miss. What is the reason for yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MEDICATIONS</td>\n",
       "      <td>Prescribed medications were Salmeterol inhaler...</td>\n",
       "      <td>Doctor: Are you taking any over the counter me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CC</td>\n",
       "      <td>Burn, right arm.</td>\n",
       "      <td>Doctor: Hi, how are you? \\r\\nPatient: I burned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>PASTSURGICAL</td>\n",
       "      <td>Vasectomy.</td>\n",
       "      <td>Doctor: Good morning, sir. \\r\\nPatient: Good m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1197</td>\n",
       "      <td>MEDICATIONS</td>\n",
       "      <td>Tylenol #3 q6h prn, ibuprofen 800 mg q8h prn, ...</td>\n",
       "      <td>Doctor: Okay, so let's go over your medication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1198</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>This patient presents to the office today for ...</td>\n",
       "      <td>Doctor: How are you doing today, sir? \\r\\nPati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1199</td>\n",
       "      <td>FAM/SOCHX</td>\n",
       "      <td>No tobacco, alcohol or illicit drug use. Patie...</td>\n",
       "      <td>Doctor: Hi, how's it going? \\r\\nPatient: Not t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>1200</td>\n",
       "      <td>FAM/SOCHX</td>\n",
       "      <td>He does not smoke.  He lives in a senior citiz...</td>\n",
       "      <td>Doctor: Looks like the nurse came in and asked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID section_header                                       section_text  \\\n",
       "0        0          GENHX  The patient is a 76-year-old white female who ...   \n",
       "1        1          GENHX  The patient is a 25-year-old right-handed Cauc...   \n",
       "2        2          GENHX  This is a 22-year-old female, who presented to...   \n",
       "3        3    MEDICATIONS  Prescribed medications were Salmeterol inhaler...   \n",
       "4        4             CC                                   Burn, right arm.   \n",
       "...    ...            ...                                                ...   \n",
       "1196  1196   PASTSURGICAL                                         Vasectomy.   \n",
       "1197  1197    MEDICATIONS  Tylenol #3 q6h prn, ibuprofen 800 mg q8h prn, ...   \n",
       "1198  1198          GENHX  This patient presents to the office today for ...   \n",
       "1199  1199      FAM/SOCHX  No tobacco, alcohol or illicit drug use. Patie...   \n",
       "1200  1200      FAM/SOCHX  He does not smoke.  He lives in a senior citiz...   \n",
       "\n",
       "                                               dialogue  \n",
       "0     Doctor: What brings you back into the clinic t...  \n",
       "1     Doctor: How're you feeling today?  \\r\\nPatient...  \n",
       "2     Doctor: Hello, miss. What is the reason for yo...  \n",
       "3     Doctor: Are you taking any over the counter me...  \n",
       "4     Doctor: Hi, how are you? \\r\\nPatient: I burned...  \n",
       "...                                                 ...  \n",
       "1196  Doctor: Good morning, sir. \\r\\nPatient: Good m...  \n",
       "1197  Doctor: Okay, so let's go over your medication...  \n",
       "1198  Doctor: How are you doing today, sir? \\r\\nPati...  \n",
       "1199  Doctor: Hi, how's it going? \\r\\nPatient: Not t...  \n",
       "1200  Doctor: Looks like the nurse came in and asked...  \n",
       "\n",
       "[1201 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dir = 'data/raw'\n",
    "processed_dir = 'data/processed'\n",
    "\n",
    "# Preview dataset\n",
    "pd.read_csv(os.path.join(raw_dir, 'MTS-Dialog-TrainingSet.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘Smaug-34B-v0.1_Q2_K.gguf’ already there; not retrieving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "from_string grammar:\n",
      "root ::= stringlist \n",
      "stringlist ::= [[] ws string [,] ws string [,] ws string ws []] \n",
      "ws ::= ws_3 \n",
      "ws_3 ::= [ <U+0009><U+000A>] ws_3 | \n",
      "string ::= [\"] string_5 [\"] \n",
      "string_5 ::= string_6 \n",
      "string_6 ::= [A-Za-z0-9 .?!,':;] string_6 | \n",
      "\n",
      "from_string grammar:\n",
      "root ::= numberlist \n",
      "numberlist ::= [[] ws number [,] ws number [,] ws number ws []] \n",
      "ws ::= ws_3 \n",
      "ws_3 ::= [ <U+0009><U+000A>] ws_3 | \n",
      "number ::= number_5 \n",
      "number_5 ::= number_6 \n",
      "number_6 ::= [1-3] number_6 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-31f1a20b-b89f-4ee7-a39c-74de762cecf2',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711929398,\n",
       " 'model': 'Smaug-34B-v0.1_Q2_K.gguf',\n",
       " 'choices': [{'text': '\\nThe color of the sky comes from scattered sunlight. This scattering effect occurs because our atmosphere has particles in it such as dust and water droplets. These small particles cause sunlight to scatter before reaching our eyes. The blue wavelengths of light are scattered more than other colors due to their shorter wavelengths, resulting in a perceived blue hue in the sky.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 69, 'total_tokens': 79}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Expert model\n",
    "!wget -nc -O \"Smaug-34B-v0.1_Q2_K.gguf\" \"https://huggingface.co/nold/Smaug-34B-v0.1-GGUF/resolve/main/Smaug-34B-v0.1_Q2_K.gguf?download=true\"\n",
    "\n",
    "expert_model = None\n",
    "expert_model = Llama(\n",
    "    \"Smaug-34B-v0.1_Q2_K.gguf\",\n",
    "    numa=True,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "string_list_grammar = LlamaGrammar.from_file(\n",
    "    \"required_string_list.grammar\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "number_list_grammar = LlamaGrammar.from_file(\n",
    "    \"required_number_list.grammar\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "expert_model(\"Objectively, why is the sky blue?\", max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "def postprocess_text(text):\n",
    "    text = re.sub(r'(\\s){2,}', r\"\\1\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def postprocess_conversation(conversation):\n",
    "    conversation = \"\\n\".join(conversation)\n",
    "    conversation = postprocess_text(conversation)\n",
    "    return conversation\n",
    "\n",
    "def postprocess_question(question):\n",
    "    if question.lower().startswith(\"doctor: \"):\n",
    "        question = question[7:]\n",
    "    question = postprocess_text(question)\n",
    "    return question\n",
    "\n",
    "def create_question_prompt(context, question):\n",
    "    if question.endswith(\"?\"):\n",
    "        return f\"\"\"\n",
    "Using the below context, give some examples of what the doctor might want to say or ask the patient after the existing conversation.\n",
    "Do not repeat any questions already asked.\n",
    "Produce as many questions as you can think of.\n",
    "For example, the doctor may say: \"{question}\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Questions:\n",
    "\"\"\".strip()\n",
    "    else:\n",
    "        return f\"\"\"\n",
    "Using the below context, give some examples of what the doctor might want to say or ask the patient after the existing conversation.\n",
    "Do not repeat any questions already asked.\n",
    "Produce as many questions as you can think of.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Questions:\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_rerank_prompt(context, questions):\n",
    "    return f\"\"\"\n",
    "Using the below context, rerank the questions in order of relevance to the conversation. The most relevant question should come first.\n",
    "Output a list of indexes, with the first and most relevant question being 1, the second being 2, and so on.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Order:\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_example(context, questions, rerank_order):\n",
    "    return f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Questions:\n",
    "{json.dumps(questions)}\n",
    "\n",
    "Ranked:\n",
    "{json.dumps(rerank_order)}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprocessing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0e0c9b15f14135a91452296230afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing MTS-Dialog-TrainingSet.csv:   0%|          | 0/1201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any know drug allergies? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is he currently taking any medication? \\r', 'Guest_family: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any allergies I should know about? \\r', 'Patient: Nope, no allergies for me.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: How did the patient do on the activity test?\\r', 'Guest_clinician: Patient was good. I have advised him to continue with his normal activities as long as he is feeling fine.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have a history of mental illness or psychological disease? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Tell me about your family medical history. \\r', \"Patient: My family is pretty healthy. I don't know of any health problems.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: The nurse applied antibiotic ointment and a bandage to your foot. Did she go over how to change the bandage at home? \\r', 'Patient: Yes.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any known drug allergies? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Your daughter had an exposure to the ant bait but we are not sure if she actually ingested any of it. Poison Control confirmed that even if she did ingest the small amount that she was found with, it is likely nontoxic. Do you have any questions? \\r', 'Guest_family: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you know anyone in your family whose had cancer? \\r', 'Patient: No one that I know.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you on any medications?\\r', 'Patient: No, not taking any medications.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: How are you dealing with your pain now? \\r', \"Patient: Oh no, I have no issues with my pain. Today I'm here for a different cause. I'm having this weakness all around on my left side, especially in my arm. I feel like I cannot lift anything like even a glass of water. It's hard for me to lift it up. I'm not able to walk properly, let alone the balancing and everything else. It's just that my whole left side seems to be weak.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history?  \\r', 'Patient: None, that I know of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor:  What's going on with you? What brings you here today? \\r\", 'Patient: I am having loose watery stools for more than two weeks now. I feel very lethargic. I also have pain in my tummy.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you allergic to anything? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke?\\r', 'Patient: No, I have never smoked in my entire life.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any past medical history? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Ever reacted poorly to any medications taken in the past? \\r', \"Patient: Hm I don't think so. No reactions so far.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you taking any medications?\\r', 'Patient: Yes I am taking thyroid meds and Lipitor.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any medications I should know about? \\r', 'Patient: Nah, no daily meds.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any past medical problems or surgeries that I should know of? \\r', 'Patient: Nope, none that I can think of.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you had any procedures in the past?\\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you take any daily medications or supplements? \\r', 'Patient: Nope.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is anyone else at home sick? \\r', \"Patient: No. My husband and daughter seem to be doing okay. They haven't complained of anything yet.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Tell me about your family medical history. \\r', 'Patient: No one in my family has any health problems.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So, I think we need to start you on antibiotics for that ulcer on your right foot and possible osteomyelitis. I will send a prescription to your pharmacy. \\r', 'Patient: Sure.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Your condition is improved, and you look pretty stable. \\r', 'Patient: Yeah, I feel much better.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any allergies to medications?\\r', 'Patient: Yes. Penicillin.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is there anyone that lives with you at home? You are going to need someone to help you care for your wound. \\r', 'Patient: My wife lives at home with me.  She can help me change my bandages.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What do you use to treat your asthma? \\r', 'Patient: I use my Albuterol inhaler.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Does he have any known drug allergies? \\r', 'Guest_family: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: How about any drugs, alcohol, or tobacco? \\r', 'Patient: No, no, and no.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Okay, I want you to follow up with our hand therapist Doctor X at her A B C D office in one to two weeks. \\r', 'Patient: Okay, I will make an appointment today.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known drug allergies?  \\r', 'Patient: No, none that I know of.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Does anyone in your family have a neurological disorder? \\r', 'Patient: No, not to my knowledge.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Breath in breath out, let me tap it and see. Well, your lungs sound clear. \\r', 'Patient: Okay.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you ever visit any hospital for any kind of surgery?\\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any old or new medications I should know about? \\r', 'Patient: Nope.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I see here that you have no surgical history. Is this correct? \\r', 'Patient: Yes, indeed. At least for now. Who knows what the future will bring.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Let's see. Are you taking any medications? \\r\", 'Patient: No, none right now.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke or drink? \\r', \"Patient: Never smoked a cigarette a day in my life. As for alcohol, I'll drink socially with friends and family.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you know about any medical issues running in your family?\\r', 'Patient: Yeah, almost everyone had diabetes.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any past or present medical conditions? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke or drink alcohol?\\r', 'Patient: Nope.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medications? \\r', 'Patient: None whatsoever.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So just to double check. No past medical history? \\r', 'Patient: Yes, sir. None.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What medications are you taking? \\r', 'Patient: I take Prilosec and Tramadol when I have back pain.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any drug allergies? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Hi, I would like to physically examine your rectal area, ok?\\r', \"Patient: No, I am not comfortable with that. I don't want you to do that.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: How's the low fat and low cholesterol diet going? Have you been finding it manageable? \\r\", \"Patient: Yeah, I guess. I've tried to also reduce my salt intake to about two grams.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke? \\r', 'Patient: Nope. I quit like fifteen years ago.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Guest_clinician: Hi there! Welcome to the clinic.\\r', 'Patient: Hi! Thank you.\\r', \"Guest_clinician: I'm going to get a little background information from you today.\\r\", 'Patient: Okay. \\r', 'Guest_clinician: What are any of your medical history conditions? Past and present?\\r', \"Patient: I have Crohn's disease.\\r\", \"Guest_clinician: How long have you had Crohn's?\\r\", \"Patient: Oh, I don't know. A long time twenty five to thirty years.\\r\", 'Guest_clinician: Have you had a colonoscopy recently?\\r', 'Patient: I had one, four years ago. Do I need to get another one this year?\\r', 'Guest_clinician: I am not sure. I will ask the doctor what his recommendation is for you would be. I will let him know that you had a colonoscopy four years ago. \\r', 'Patient: Sounds good. I am also a diabetic. That has been going on for four years now.\\r', \"Guest_clinician: Yes, I see you take insulin. I also see that you're on a blood pressure medication, do you have high blood pressure?\\r\", 'Patient: Yes, I have high blood pressure and high cholesterol. I also have heart disease. I had a stent placed in there.\\r', 'Guest_clinician: Anything else?\\r', \"Patient: I have arthritis now. They said it's due to my Crohn's.\\r\", 'Guest_clinician: That makes sense.\\r', \"Patient: I'm being treated for depression now as well.\\r\", 'Guest_clinician: Okay. Any other surgeries other than the stent?\\r', \"Patient: Oh, I had a kidney stone removed several years ago. Twenty five years ago. That's it. That's all I got for you.\\r\", 'Guest_clinician: Okay, great! Thank you. I will go let the doctor know that you are ready and she will be right in to see you. \\r', \"Patient: I'll be here.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any drug allergies? \\r', 'Patient: No. None that I know of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: So, if you see here Doctor Patrick there are multiple areas with hypergranulation tissue on the left leg posteriorly and I get a sense that it's associated with the trauma to the back of his right leg. \\r\", 'Guest_clinician: Yeah, I agree.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Okay, so your psych evaluation came out unremarkable. Do you have any other problems?\\r', 'Patient: I have this acne. I hate it. I am not feeling comfortable in my own skin.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family health history? \\r', 'Patient: My mom and dad are both diabetics. My mom has high blood pressure. My brother is having some issues with his liver.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you had any surgeries in the past? \\r', 'Patient: No, none.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: How about allergies to medications? \\r', 'Patient: Nope.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is going on with him?\\r', 'Guest_family: He is complaining of chest pain.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you had any surgery in the past? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Do you have any family history of neurological disorders, such as multiple sclerosis, Parkinson's or dementia? \\r\", 'Patient: Nope. Fortunately, no one in my family has neurological problems.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any history of surgical procedures?  \\r', 'Patient: No, none.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history?\\r', 'Patient: Nothing that I know of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you use any tobacco, inhalation or recreational drugs? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any family history of disease? \\r', \"Guest_family: To be honest, I'm not sure. Possibly? I'd have to check and get back to you.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any history of surgical procedures? \\r', \"Patient: I remember having surgery as a child, but I don't know what the surgery was for. It was so long ago.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I think you are good to go home. I am going to prepare your discharge paper. \\r', \"Patient: That's great!\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I am going write you a prescription for a stimulant. It is called Cylert.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: How's your overall health? \\r\", \"Patient: Fine, I guess. I'm being treated for I B S by my G I. Last time I was in, they told me that my white blood cell levels aren't where they were supposed to be.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: So, what I feel is you might have short term memory loss due to high stress, but I also feel it's probably due to your current situation. The other impression I have is that you have anxiety issues and again it's stress related. \\r\", 'Patient: Yeah, I do have anxiety issues.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any nausea, vomiting or diarrhea?\\r', 'Patient: Nope.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Our records show that he is up to date with his vaccinations.\\r', 'Guest_family: Okay.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you allergic to any medications? \\r', 'Patient: No']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medication? \\r', 'Patient: Nope. None that I know of.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Hello, sir how are you feeling today? \\r', 'Patient: I feel terrible. Very sick and I can barely move. I feel so weak.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you currently taking any medications? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So are you allergic- \\r', 'Patient: To any medications? No allergies here.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you ever had psychiatric or psychological treatment? \\r', \"Patient: Uh no, no treatments whatsoever. I mean I do find myself in low moods every now and again when I'm dealing with certain stressors, but the decreased moods rarely ever long lasting.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any family history of heart disease? \\r', 'Patient: My mother has coronary artery disease.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Any health issues on your mom or dad's side? \\r\", \"Patient: No, we're a pretty healthy family.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke or drink? \\r', \"Patient: Nope, I've never touched a cigarette nor do I plan on ever doing so. Same goes for alcohol.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you up to date on your vaccines? \\r', \"Patient: I've actually never been vaccinated before. My parents don't believe in vaccines.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Looking at your medical history now. Any changes since your last visit? \\r', 'Patient: No changes.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is this your first surgery or have you had other surgeries in the past? \\r', 'Patient: This will be my second. I had a hernia repair back in O ten.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Have you had any adverse reactions to medications you've taken in the past? \\r\", 'Patient: No, thankfully.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any known allergies? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: So just to double check, you're not on any medications? \\r\", 'Patient: You got it. Nothing since last year.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medications?  \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What medications are you currently taking? \\r', 'Patient: Augmentin, Detroal LA and lisinopril.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you ever have any surgery?\\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you taking any medicine? \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: I don't think we need to do any major procedure right now.\\r\", 'Patient: Okay.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you have any medical issues?\\r', 'Patient: Nope.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known drug allergies? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history? \\r', \"Patient: My uncle on my dad's side of the family had kidney failure.  I lot of my family members have diabetes and asthma.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history? \\r', 'Patient: My mother had cancer.  She had multiple myeloma.  She passed away at age eighty three.  My dad died when he was eighty one.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So, you are following up for your high blood pressure and high cholesterol?\\r', 'Patient: Yes.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any past medical history I should be aware of? \\r', 'Patient: Nope.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Anyone sick at home? \\r', 'Patient: No, fortunately. Everyone else is feeling fine.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you have any surgeries in the past?\\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history? \\r', \"Patient: I don't know of any family health problems.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Smoke or drink? \\r', 'Patient: No and no.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Your bloodwork came back from the Coumadin sensitivity test. The results showed a high level of sensitivity. I am going to adjust your dosage of Coumadin to a much lower dose. This will help reduce the bruising and the bleeding episodes. Do you have any questions?\\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you use tobacco, alcohol, or recreational drugs? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medications?  \\r', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any past medical history? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history? \\r', 'Patient: I am not sure. Everyone is healthy as far as I know.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: You can take Ibuprofen as needed for pain.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any known drug allergies? \\r', 'Patient: Nope, none whatsoever.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: How did your treatment for the thrombosis go? I believe we did it on February nineteen of two thousand and seven. We finished the note the next day in order to send to your other following doctor. \\r', ' Patient: The treatment went well. Thanks!']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you smoke cigarettes?\\r', 'Patient: Yes.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Can you please confirm the medications that you have already taken?\\r', 'Patient: Yeah, I just finished the course for Minocin. I also took Duraphen II D M.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have a family history of any medical conditions, such as diabetes or heart disease? \\r', 'Patient: No, nothing.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your family medical history?  \\r', \"Patient: I was adopted.  I don't know my family medical history.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any major or minor surgeries done in the past?\\r', 'Guest_family: Sorry, I have no idea about it.\\r', 'Guest_family: I just started working for him.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any of your family members have issues like seizures?\\r', \"Patient: None on my father's side, but, yeah, on my mom's side some of the family members had seizures. Like, my mom had them during her childhood. I know that my great grandmother had it and so does my great aunt.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have a sore throat, cough or earache? \\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any medical problems that run in the family? \\r', 'Patient: No, nothing that comes to mind.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any issues from head to toe?\\r', 'Patient: No, everything is fine.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Tell me about your family medical history. \\r', 'Patient: No one in my family has any health problems.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Okay, so where do you live?\\r', 'Patient: I live at an old age nursing home.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I have reviewed your x rays from your emergency room visit. I agree with Doctor Jones that your lungs show no signs of infection or decreased lung capacity. \\r', 'Patient: Okay.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Tell me about your family medical history. \\r', 'Patient: The only thing that I know about is that my grandma has diabetes.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: History of any illnesses, surgeries, or hospitalizations? \\r', \"Patient: I dislocated my wrist when I was a kid but that's all.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is there any history of heart disease in your family? \\r', 'Patient: Yes, my dad and brother had heart vessel blockage type of disease in their forties.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Well, I am looking at your ultrasound report and honestly the only significant finding of this area is that it shows to be related to bone. \\r', 'Patient: Oh okay.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do thyroid problems run in your family?\\r', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Have you ever had an allergic reaction to drugs you've taken in the past? \\r\", \"Patient: More than once. I've reacted poorly to sulfa drugs, penicillin, and some mycins. I also think I'm allergic to contrast medium.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any history of major surgeries? \\r', 'Patient: No. Nothing major.']\n",
      "Invalid questions result\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d20cde52fde4ae9ad9824f4152bea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing MTS-Dialog-ValidationSet.csv:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you been experiencing any mental difficulties or confusion? ', 'Patient: No. Doctor: Any hallucinations? Are you seeing hearing thing that is not real? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Who are going to stay with? ', 'Patient: I am going home with my son. I will stay with him.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Let me write you a prescription for Cipro and Flagyl.', 'Patient: Okay.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What is your surgical history? ', 'Patient: I had cataract surgery on both eyes. I also had knee replacement surgery on my left knee.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I have sent over your referral for physical therapy, occupational therapy and speech therapy. The patient coordinator at Siskin Rehab Hospital will give you a call within two days to see up your appointments. It is important to take your medications regularly. You need to continue the Cipro for an additional two days only. That will take care of the U T I. Do you have any questions? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Does she have any past medical history or health problems? ', 'Guest_family: No. She is a healthy child.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Let me add Flagyl intra venously. It will be five hundred M G every eight hours and I would also like to add Levaquin five hundred M G daily. ', 'Patient: Okay.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have a history of tobacco, alcohol or recreational drug use? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you had any surgery in the past? ', 'Patient: Yes, I had this major trauma surgery some time back.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you allergic to anything, food or medicines?', 'Patient: No allergies that I know of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any past surgeries? ', 'Patient: Nah.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I reviewed all your systems, everything looks fine. ', 'Patient: Nice.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you know about any medical problems running in your family? ', \"Patient: No, I don't know anything. I was never close to my family.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any medication intolerances? ', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: And you mentioned that you have a history of migraine?', \"Patient: Yes, that's correct.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What medications are you taking currently? ', \"Patient: Well, I'm taking Remeron for depression and Ziac for high blood pressure.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Good news! No need for any shots today. He is up to date on his immunizations. ', 'Guest_family: Good! He hates shots.']\n",
      "Invalid questions result\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60143edf55e490f8470ac28c6b47953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing MTS-Dialog-TestSet-2-MEDIQA-Sum-2023.csv:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known history of allergies?', \"Patient: No, I don't have any allergies.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I belive you caught a virus.  You have the stomach flu.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any known allergies? ', 'Patient: Um none that I can think of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you ever had any surgeries? ', 'Patient: I had my appendix removed when I was nine years old.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What brings you in today? ', 'Patient: I was at the beach walking along the rocks. My foot slipped and I cut my foot on the barnacles.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Hi there, so tell me what's going on?\", \"Patient: I have realized lately, I can't walk as much and as far I used to before.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [11] \n",
      "Turns: ['Doctor: Hi mom, this is Doctor Jay. He is a pediatric specialist, and he will take a look at the baby. ', 'Guest_clinician: Hi there, how are you?', 'Guest_family: I am a little scared!', \"Guest_clinician: Don't worry, we will take care of her. How old is she?\", 'Guest_family: She is ten days old. ', 'Guest_clinician: And what did you notice? ', 'Guest_family: Well, I noticed these unusual jerky movement in her hand and leg. Like they suddenly start shaking.', 'Guest_clinician: Kind of seizure like?', 'Guest_family: Yeah.', 'Guest_clinician: Anything else going on with baby?', 'Guest_family: Hm, she was detected with infant jaundice when she was born. ', 'Doctor: Yeah, baby had physiological jaundice. ', 'Guest_clinician: Okay.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medications? ', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any changes to your family history since your last visit? ', 'Patient: No, no changes.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you taking any medications currently?', 'Patient: Um just Ibuprofen for my pain.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: Ma'am, can you tell me how long have you been having this pain? \", 'Patient: Um, can you?', \"Guest_family: Oh, she doesn't remember anything doctor. I tried asking her, but she could not answer.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any medical concerns or issues in the past?', 'Patient: No.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any allergies to medications? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: So Mister X, you don't smoke, do you? \", 'Patient: Nope, never picked up a cig my whole life. ', 'Guest_family: I wish I could say the same. ', 'Patient: Yeah, my wife has picked up one too many if you ask me.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Besides cleft palate, any other past medical history I should know of? ', 'Patient: Yeah, nothing else. Just that.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you know anyone in your family who has suffered from any cancers or blood disorders? ', 'Patient: Not to my knowledge.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So other than acne, no medical problems? ', 'Patient: Correct.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: [\"Doctor: What's bringing you in today? \", \"Patient: I've lost a lot of weight and I wanna make sure everything is okay.\"]\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known allergies to medications? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What medications are you currently taking? ', 'Patient: Just birth control pills.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any medical problems?  ', 'Patient: Um medical problems? No, thank God.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you on any medications? ', 'Patient: I was taking Augmentin for a U T I but I am done with the course now.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So, tell me what is going on?', 'Patient: I am coughing blood!']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any known drug allergies? ', 'Patient: No, none.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any past or present medical conditions? ', 'Patient: I have high blood pressure and I am a type two diabetic.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ca538addb043e4927cb4745a07620d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you know about any allergies from any medications?', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Guest_clinician: Hello, my name is Mary. I will ask you a few questions about your medical and family history and then Doctor Smith will come and check you. Okay?', 'Patient: Okay. ', 'Guest_clinician: Do you have any other previously diagnosed medical issues?', 'Patient: I have sinus. I also had a stroke around two years ago.', 'Guest_clinician: Do you smoke or drink?', 'Patient: Nope, never did any of those.', 'Guest_clinician: Do you have any kind of allergies?', 'Patient: No, no known allergies.', 'Guest_clinician: Thank you for answering all my questions, I will let Doctor Smith know that you are ready.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: I spoke with Poison Control regarding the possible ingestion of the liquid. They let me know that it is actually a relatively small amount and is likely to be a nontoxic ingestion of the liquid, if she did end up ingesting it. It is not likely to be the case as she is behaving as if she did not ingest any of the liquid.', 'Guest_family: Thank god! Thank you.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: So, just to confirm, no current medications? ', \"Patient: Yes, that's right.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: What are your current medications?', 'Patient: None, except the Amoxil and Aldex that I started on Monday. ']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any family medical history? ', \"Patient: No. I don't know of any family health problems.\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: That is just normal male breast tissue, nothing to worry about.', 'Patient: Oh okay.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you ever have had any surgery? ', 'Patient: Not that I remember.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known drug allergies? ', 'Patient: No.  None that I know of.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you experiencing any symptoms? ', 'Patient: Nope.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you taking any medication? ', 'Patient: Just multivitamins and calcium.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any known drug allergies? ', 'Patient: No.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any allergies? ', 'Patient: I am not allergic to anything as far as I know.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: And we did review your family history last time. No one else suffers from heart problems in your family, right? ', \"Patient: Yeah, it's just me.\"]\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Is this your first pregnancy? ', \"Patient: Yes. I'm so excited for the baby to arrive!\"]\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Are you still take Prevacid? ', 'Patient: No. I had to stop taking it.  It gave me diarrhea.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Looks like he has a foreign body in his left eye. ', 'Guest_family: Does he need to see an eye doctor after this?']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any history of tobacco, alcohol, or drug use? ', 'Patient: I have a glass of wine with dinner every once in a while, but no other than that.']\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Did you have any known allergies?', 'Patient: Not that I know.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Welcome to the clinic.  Let start with your medical history. ', 'Patient: I was diagnosed with Parkinson disease by Doctor Johnson five years ago.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Can you tell me if you ever had chronic issues? Like any illnesses?', 'Patient: Let me think, um, yeah they took my tonsils out for recurrent infections.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Any allergies to any drugs?', 'Patient: Ah, yes, what is called, cepro-', 'Guest_family: Oh, you are allergic to Cephalexin mother! ', 'Patient:  Yep, thank you honey.']\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Do you have any medical conditions or illnesses?', 'Patient: I have asthma.']\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Invalid questions result\n",
      "Not enough doctor indexes: [] \n",
      "Turns: ['Doctor: Have you had your flu or Pneumonia vaccination this year? ', 'Patient: I got my flu shot this year, but I did not get the one for Pneumonia.']\n"
     ]
    }
   ],
   "source": [
    "# Remove existing processed data\n",
    "if os.path.exists(processed_dir) and len(os.listdir(processed_dir)):\n",
    "    # ipynb prompt user if they want to reprocess data\n",
    "    do_process_data = input(\"Processed data already exists, do you want to reprocess data? (y/n): \").lower() == 'y'\n",
    "    if do_process_data:\n",
    "        print(\"Reprocessing data...\")\n",
    "else:\n",
    "    do_process_data = True\n",
    " \n",
    "if do_process_data:\n",
    "    shutil.rmtree(processed_dir, ignore_errors=True)\n",
    "    os.makedirs(processed_dir, exist_ok=False)\n",
    "\n",
    "    for data_source in os.listdir(raw_dir):\n",
    "        if not data_source.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(os.path.join(raw_dir, data_source))\n",
    "\n",
    "        # Process dataset\n",
    "        processed_data = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=f'Processing {data_source}'):\n",
    "            examples = []\n",
    "            \n",
    "            # Get dialogue text and convert to turns\n",
    "            turns = row['dialogue'].split(\"\\n\")\n",
    "            num_examples = max((len(turns) - 2) // 5, 1)\n",
    "            \n",
    "            # Get a list of indexes in turns where the sentence starts with \"doctor:\"\n",
    "            doctor_indexes = [i for i, turn in enumerate(turns) if turn.lower().startswith(\"doctor: \") and i > 0]\n",
    "            \n",
    "            # Get num_examples of doctor indexes\n",
    "            try:\n",
    "                doctor_indexes = random.sample(doctor_indexes, num_examples)\n",
    "            except:\n",
    "                print(\"Not enough doctor indexes:\", doctor_indexes, \"\\nTurns:\", turns)\n",
    "                pass\n",
    "            \n",
    "            for i in doctor_indexes:\n",
    "                # Extract and cleanup conversation\n",
    "                context = postprocess_conversation(turns[0:i])\n",
    "                example = postprocess_question(turns[i])\n",
    "                \n",
    "                prompt = create_question_prompt(context, example)                \n",
    "                try:\n",
    "                    expert_generated_questions = expert_model(prompt, max_tokens=100, grammar=string_list_grammar)\n",
    "                    expert_generated_questions = json.loads(expert_generated_questions[\"choices\"][0][\"text\"])\n",
    "                except:\n",
    "                    print(\"Invalid questions result\")\n",
    "                    continue\n",
    "                \n",
    "                prompt = create_rerank_prompt(context, expert_generated_questions)\n",
    "                try:\n",
    "                    expert_generated_rerank = expert_model(prompt, max_tokens=50, grammar=number_list_grammar)                \n",
    "                    expert_generated_rerank = json.loads(expert_generated_rerank[\"choices\"][0][\"text\"])\n",
    "                except:\n",
    "                    print(\"Invalid rerank result\")\n",
    "                    continue\n",
    "                \n",
    "                example = create_example(context, expert_generated_questions, expert_generated_rerank)\n",
    "                processed_data.append({\n",
    "                    'text': example,\n",
    "                })\n",
    "\n",
    "                # Save processed data\n",
    "                pd.DataFrame(processed_data).to_csv(os.path.join(processed_dir, data_source), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Context:\\nDoctor: What brings you back into th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Context:\\nDoctor: How're you feeling today?\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context:\\nDoctor: How're you feeling today?\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Context:\\nDoctor: Hello, miss. What is the rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Context:\\nDoctor: Are you taking any over the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>Context:\\nDoctor: How are you doing today, sir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Context:\\nDoctor: Hi, how's it going?\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>Context:\\nDoctor: Hi, how's it going?\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>Context:\\nDoctor: Hi, how's it going?\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>Context:\\nDoctor: Looks like the nurse came in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1702 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Context:\\nDoctor: What brings you back into th...\n",
       "1     Context:\\nDoctor: How're you feeling today?\\nP...\n",
       "2     Context:\\nDoctor: How're you feeling today?\\nP...\n",
       "3     Context:\\nDoctor: Hello, miss. What is the rea...\n",
       "4     Context:\\nDoctor: Are you taking any over the ...\n",
       "...                                                 ...\n",
       "1697  Context:\\nDoctor: How are you doing today, sir...\n",
       "1698  Context:\\nDoctor: Hi, how's it going?\\nPatient...\n",
       "1699  Context:\\nDoctor: Hi, how's it going?\\nPatient...\n",
       "1700  Context:\\nDoctor: Hi, how's it going?\\nPatient...\n",
       "1701  Context:\\nDoctor: Looks like the nurse came in...\n",
       "\n",
       "[1702 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview processed dataset\n",
    "pd.read_csv(os.path.join(processed_dir, 'MTS-Dialog-TrainingSet.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup memory\n",
    "del expert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We use [Tinyllama](https://github.com/jzhang38/TinyLlama), a small 1.1b param autoregressive model based on the LLAMA architecture. This size is chosen for quick trianing and fast evaluation.\n",
    "\n",
    "We also use [Unsloth](https://github.com/unslothai/unsloth) to perform QLora operations on the model, loowing for even faster training and more efficient memory usage.\n",
    "\n",
    "Training required 24gb of vram, 2gb of system ram, and was completed on a RTX 3090. Training took approximately 12 minutes, and concluded early when validation loss started to degrade at about 3.3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-hyperparam configuration\n",
    "output_dir = \"models/ramp1\"\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "is_lora = True\n",
    "resume_from = None # Set if we want to resume from a checkpoint\n",
    "val_size = 0.1 # This is only used if valid_path is None\n",
    "train_path = os.path.join(processed_dir, 'MTS-Dialog-TrainingSet.csv')\n",
    "valid_path = os.path.join(processed_dir, 'MTS-Dialog-ValidationSet.csv')\n",
    "save_steps = 25\n",
    "logging_steps = 5\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"CS598\"\n",
    "\n",
    "# Hyperparameters\n",
    "epochs=10\n",
    "lr_scheduler_type = \"cosine\"\n",
    "warmup_ratio = 0.1\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.1\n",
    "gradient_accumulation_steps = 2\n",
    "optimizer = \"adamw_8bit\"\n",
    "use_gradient_checkpointing = True\n",
    "random_state = 42\n",
    "final_output_model_dir = os.path.join(output_dir, \"final\")\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = False  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sdome helper functions\n",
    "def merge_and_save(output_path=\"/tmp/merged\"):\n",
    "    global model\n",
    "    global tokenizer\n",
    "\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(output_path, safe_serialization=False)\n",
    "    if tokenizer is not None:\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 24.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.23. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer.json\n",
      "loading file tokenizer.model from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/tokenizer_config.json\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "tokenizer config file saved in _unsloth_sentencepiece_temp/TinyLlama_TinyLlama-1.1B-intermediate-step-1431k-3T/tokenizer_config.json\n",
      "Special tokens file saved in _unsloth_sentencepiece_temp/TinyLlama_TinyLlama-1.1B-intermediate-step-1431k-3T/special_tokens_map.json\n",
      "tokenizer config file saved in _unsloth_sentencepiece_temp/TinyLlama_TinyLlama-1.1B-intermediate-step-1431k-3T/tokenizer_config.json\n",
      "Special tokens file saved in _unsloth_sentencepiece_temp/TinyLlama_TinyLlama-1.1B-intermediate-step-1431k-3T/special_tokens_map.json\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model directory.\n",
      "Creating LORA model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.3 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "#  Load / resume model\n",
    "global model\n",
    "global tokenizer\n",
    "model, tokenizer = FastLlamaModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "if (os.path.exists(output_dir) and len(os.listdir(output_dir)) > 1) or (\n",
    "    resume_from is not None\n",
    "):\n",
    "    if resume_from is not None:\n",
    "        latest_checkpoint = resume_from\n",
    "    else:\n",
    "        latest_checkpoint_steps = max(\n",
    "            [\n",
    "                int(ckpt.split(\"-\")[1])\n",
    "                for ckpt in os.listdir(output_dir)\n",
    "                if ckpt.startswith(\"checkpoint\")\n",
    "            ]\n",
    "        )\n",
    "        latest_checkpoint = os.path.join(\n",
    "            output_dir, f\"checkpoint-{latest_checkpoint_steps}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Loading model from checkpoint-{latest_checkpoint}.\")\n",
    "\n",
    "    if \"adapter_config.json\" in os.listdir(latest_checkpoint):\n",
    "        print(\"adapter_config.json` found. Loading as a PeftModel.\")\n",
    "        model = PeftModel.from_pretrained(model, latest_checkpoint)\n",
    "\n",
    "        model, tokenizer = FastLlamaModel.from_pretrained(\n",
    "            model_name=merge_and_save(),\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "\n",
    "        is_lora = True\n",
    "    else:\n",
    "        print(\"adapter_config.json` not found. Loading as a FastLlamaModel.\")\n",
    "        model, tokenizer = FastLlamaModel.from_pretrained(\n",
    "            model_name=latest_checkpoint,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "\n",
    "        is_lora = False\n",
    "else:\n",
    "    print(\"Creating new model directory.\")\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "if is_lora or (is_lora == False and resume_from is not None):\n",
    "    print(\"Creating LORA model.\")\n",
    "    model = FastLlamaModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0,  # Currently only supports dropout = 0\n",
    "        bias=\"none\",  # Currently only supports bias = \"none\"\n",
    "        use_gradient_checkpointing=use_gradient_checkpointing,  # With Unsloth, we can turn this off!\n",
    "        random_state=random_state,\n",
    "        use_rslora=True,  # We support rank stabilized LoRA\n",
    "        loftq_config=LoftQConfig(loftq_bits=4),  # And LoftQ\n",
    "        max_seq_length=max_seq_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>Context:\\nDoctor: Hello, sir. Before we begin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Context:\\nDoctor: Hello. How are you doing?\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>Context:\\nDoctor: Did you ever had pneumonia?\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Context:\\nDoctor: I have their surgical histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Context:\\nDoctor: Welcome to the clinic.\\nPati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>Context:\\nDoctor: Hello sir, how are you?\\nPat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>Context:\\nDoctor: Hello, how are you?\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Context:\\nDoctor: What types of surgeries have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Context:\\nDoctor: Has anyone in your family ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Context:\\nDoctor: You can continue working, I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1702 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "705   Context:\\nDoctor: Hello, sir. Before we begin ...\n",
       "809   Context:\\nDoctor: Hello. How are you doing?\\nP...\n",
       "1432  Context:\\nDoctor: Did you ever had pneumonia?\\...\n",
       "173   Context:\\nDoctor: I have their surgical histor...\n",
       "513   Context:\\nDoctor: Welcome to the clinic.\\nPati...\n",
       "...                                                 ...\n",
       "1130  Context:\\nDoctor: Hello sir, how are you?\\nPat...\n",
       "1294  Context:\\nDoctor: Hello, how are you?\\nPatient...\n",
       "860   Context:\\nDoctor: What types of surgeries have...\n",
       "1459  Context:\\nDoctor: Has anyone in your family ha...\n",
       "1126  Context:\\nDoctor: You can continue working, I ...\n",
       "\n",
       "[1702 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv(train_path).sample(frac=1, random_state=random_state)\n",
    "valid = None\n",
    "\n",
    "if valid_path is not None:\n",
    "    valid = pd.read_csv(valid_path).sample(frac=1, random_state=random_state)\n",
    "else:\n",
    "    train, valid = train.train_test_split(test_size=val_size, seed=random_state)\n",
    "    \n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab63e52c8c042928658b016685fbddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abf15b0c5894f1b93cb0f52cc3a2efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,702 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 32 | Total steps = 530\n",
      " \"-____-\"     Number of trainable parameters = 25,231,360\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 24.0 GB.\n",
      "2.246 GB of memory reserved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjef1056\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jfan/CS-598-Final/wandb/run-20240331_200119-vkb7ja1t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jef1056/CS598/runs/vkb7ja1t/workspace' target=\"_blank\">quiet-haze-2</a></strong> to <a href='https://wandb.ai/jef1056/CS598' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jef1056/CS598' target=\"_blank\">https://wandb.ai/jef1056/CS598</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jef1056/CS598/runs/vkb7ja1t/workspace' target=\"_blank\">https://wandb.ai/jef1056/CS598/runs/vkb7ja1t/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/530 09:44 < 19:58, 0.30 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.697500</td>\n",
       "      <td>1.747466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.408900</td>\n",
       "      <td>1.451734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.303500</td>\n",
       "      <td>1.403583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.223100</td>\n",
       "      <td>1.381681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.171700</td>\n",
       "      <td>1.381397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.169800</td>\n",
       "      <td>1.385434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.091400</td>\n",
       "      <td>1.404944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-25\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-25/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-25/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-50\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-50/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-75\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-75/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [models/ramp1/checkpoint-25] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-125\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-125/special_tokens_map.json\n",
      "Deleting older checkpoint [models/ramp1/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-150\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [models/ramp1/checkpoint-75] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to models/ramp1/checkpoint-175\n",
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/checkpoint-175/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/checkpoint-175/special_tokens_map.json\n",
      "Deleting older checkpoint [models/ramp1/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/ramp1/checkpoint-125 (score: 1.3813966512680054).\n"
     ]
    }
   ],
   "source": [
    "# Define and run training loop\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=Dataset.from_pandas(train),\n",
    "    eval_dataset=Dataset.from_pandas(valid),\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_num_proc=multiprocessing.cpu_count() // 2,\n",
    "    args=TrainingArguments(\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        tf32=True,\n",
    "        dataloader_num_workers=4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=not HAS_BFLOAT16,\n",
    "        bf16=HAS_BFLOAT16,\n",
    "        output_dir=output_dir,\n",
    "        optim=optimizer,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        seed=random_state,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        logging_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        save_safetensors=False,\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(2, 0.0)],\n",
    ")\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jfan/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-intermediate-step-1431k-3T/snapshots/036fa4651240b9a1487f709833b9e4b96b4c1574/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in models/ramp1/final/lora/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/final/lora/special_tokens_map.json\n",
      "Configuration saved in models/ramp1/final/merged/config.json\n",
      "Configuration saved in models/ramp1/final/merged/generation_config.json\n",
      "Model weights saved in models/ramp1/final/merged/pytorch_model.bin\n",
      "tokenizer config file saved in models/ramp1/final/merged/tokenizer_config.json\n",
      "Special tokens file saved in models/ramp1/final/merged/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "shutil.rmtree(final_output_model_dir, ignore_errors=True)\n",
    "if is_lora or (is_lora == False and resume_from is not None):\n",
    "    os.makedirs(os.path.join(final_output_model_dir, \"lora\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(final_output_model_dir, \"merged\"), exist_ok=True)\n",
    "    model.save_pretrained(\n",
    "        os.path.join(final_output_model_dir, \"lora\"), safe_serialization=False\n",
    "    )\n",
    "    tokenizer.save_pretrained(os.path.join(final_output_model_dir, \"lora\"))\n",
    "    merge_and_save(os.path.join(final_output_model_dir, \"merged\"))\n",
    "else:\n",
    "    model.save_pretrained(final_output_model_dir, safe_serialization=False)\n",
    "    tokenizer.save_pretrained(final_output_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to evaluate this model, we complute the ROGUE score of the model's outputs against the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/ramp1/final/merged/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"models/ramp1/final/merged\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"unsloth_version\": \"2024.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file models/ramp1/final/merged/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"models/ramp1/final/merged\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"unsloth_version\": \"2024.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file models/ramp1/final/merged/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 24.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.23. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at models/ramp1/final/merged.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file models/ramp1/final/merged/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "tokenizer config file saved in _unsloth_sentencepiece_temp/models_ramp1_final_merged/tokenizer_config.json\n",
      "Special tokens file saved in _unsloth_sentencepiece_temp/models_ramp1_final_merged/special_tokens_map.json\n",
      "tokenizer config file saved in _unsloth_sentencepiece_temp/models_ramp1_final_merged/tokenizer_config.json\n",
      "Special tokens file saved in _unsloth_sentencepiece_temp/models_ramp1_final_merged/special_tokens_map.json\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "random_state = 42\n",
    "\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()\n",
    "\n",
    "model, tokenizer = FastLlamaModel.from_pretrained(\n",
    "    model_name = os.path.join(final_output_model_dir, \"merged\"),\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    ")\n",
    "\n",
    "# model.save_pretrained_gguf(\"final_model.gguf\", tokenizer)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e888a9825e24a6b91e731956f589b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge1 F1 Score: 0.29979520974440355\n",
      "Average rouge2 F1 Score: 0.11813238381054471\n",
      "Average rougeL F1 Score: 0.23140387876731552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ebfa3a3d8d4e73b55585292017d2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge1 F1 Score: 0.13252110868218833\n",
      "Average rouge2 F1 Score: 0.051978575980848846\n",
      "Average rougeL F1 Score: 0.10097343177444587\n"
     ]
    }
   ],
   "source": [
    "test_data1 = pd.read_csv(os.path.join(processed_dir, 'MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'))\n",
    "test_data2 = pd.read_csv(os.path.join(processed_dir, 'MTS-Dialog-TestSet-2-MEDIQA-Sum-2023.csv'))\n",
    "\n",
    "score_counters = {}\n",
    "processed_count = 0\n",
    "total = len(test_data1)\n",
    "\n",
    "# iterate through test data\n",
    "for test_dataset in [test_data1, test_data2]:\n",
    "    for i, row in tqdm(test_dataset.iterrows(), total=total):\n",
    "        prompt, answers = row['text'].split(\"Questions:\")\n",
    "        inputs = prompt + \"Questions:\"\n",
    "        inputs_tokenized = tokenizer([inputs], return_tensors = \"pt\").to(\"cuda\")\n",
    "        outputs = \"\\n\\n\".join(tokenizer.decode(model.generate(**inputs_tokenized, max_new_tokens = 128, use_cache = False, top_p = 0.9, top_k = 50, temperature = 0.9, num_return_sequences = 1)[0])[len(inputs)+5:].split(\"\\n\\n\")[0:2]).strip()\n",
    "\n",
    "        scores = scorer.score(row['text'], outputs)\n",
    "        \n",
    "        for key in scores:\n",
    "            if key not in score_counters:\n",
    "                score_counters[key] = 0\n",
    "            score_counters[key] += scores[key].fmeasure\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        if i == total:\n",
    "            break\n",
    "\n",
    "    for key in score_counters:\n",
    "        score_counters[key] /= processed_count\n",
    "        print(f\"Average {key} F1 Score: {score_counters[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Context:\n",
      "Doctor: Hi there! I am Doctor Jones, sir.\n",
      "Patient: Hello! It is nice to meet you.\n",
      "Doctor: What brings you into see me today?\n",
      "Patient: I have had this weakness in my right leg for quite some time now.\n",
      "Doctor: How long has this been going on and do you know how you injured yourself?\n",
      "Patient: I think that it was about six months ago that the weakness in my leg started. I don't really remember how it happened.\n",
      "Doctor: Can you tell me what you do remember?\n",
      "Patient: I was reaching to get something from a cabinet, and I noticed that I was unable to stand on my right toe. Ever since then I have had difficulty pushing off when walking. My toes were tingling and numb.\n",
      "Doctor: Was the numbness and tingling mild, moderate, or severe?\n",
      "Patient: It was a mild feeling, but this has been an ongoing problem that has been the same since the weakness started.\n",
      "Doctor: Have you had any other pain any where else in your body?\n",
      "Patient: I have had back pain, but this has been going on for many years and has not changed.\n",
      "Doctor: Is the back pain been mild, moderate, or severe?\n",
      "Patient: I would say mild. I have also been having cramping in both calves.\n",
      "Doctor: How long has that been going on?\n",
      "Patient: For the past year but it stopped about two months ago.\n",
      "\n",
      "Questions:\n",
      "[\"What type of pain are you experiencing in your right leg?\", \"Have you noticed any changes in your right leg since your last visit?\", \"Have you noticed any changes in your right leg since your last visit?\"]\n",
      "\n",
      "Ranked:\n",
      "[3, 2, 1]\n",
      "\n",
      "Questions:\n",
      "[\"How long has this been going on and do you know how you injured yourself?\", \"Can you tell me what you do remember?\", \"Have you had any other pain any where else in your body?\"]\n",
      "\n",
      "Ranked:\n",
      "[3, 2, \n"
     ]
    }
   ],
   "source": [
    "prompt, answers = test_data1['text'][40].split(\"Questions:\")\n",
    "inputs = prompt + \"Questions:\"\n",
    "inputs_tokenized = tokenizer([inputs], return_tensors = \"pt\").to(\"cuda\")\n",
    "print(tokenizer.decode(model.generate(**inputs_tokenized, max_new_tokens = 128, use_cache = False, top_p = 0.9, top_k = 50, temperature = 0.9, num_return_sequences = 1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results And Analysis\n",
    "Average ROGUE score is pretty average. Something interesting is that the model acheives a pretty low loss on the test set and remains consistent on the validation set. This can probalby be attributed to the say MSE scores vs ROGUE, and how the resulting text produced by the model might be more than rewording or a different variant of the options produced by the original expert model.\n",
    "\n",
    "Average rouge1 F1 Score: 0.29979520974440355\n",
    "Average rouge2 F1 Score: 0.11813238381054471\n",
    "Average rougeL F1 Score: 0.23140387876731552\n",
    "\n",
    "Looking at the actual results from model generation, for example:\n",
    "```text\n",
    "\n",
    "```\n",
    "The results seem to line up retty close with the test set, and while they may contain different questions thir content and the order in which they could be prioritized is, at least to human evaluation, is done suprisingly well.\n",
    "\n",
    "Model training metrics can be found here: https://wandb.ai/jef1056/CS598/\n",
    "\n",
    "# Plans\n",
    "Additional potential improvments:\n",
    "- Dividing the task better into re-ranking and generation steps\n",
    "- Hyperparam sweek to determine best training metric\n",
    "- Enhance logging and charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results And Analysis\n",
    "Average ROGUE score is pretty average. Something interesting is that the model acheives a pretty low loss on the test set and remains consistent on the validation set. This can probalby be attributed to the say MSE scores vs ROGUE, and how the resulting text produced by the model might be more than rewording or a different variant of the options produced by the original expert model.\n",
    "\n",
    "Average rouge1 F1 Score: 0.29979520974440355\n",
    "Average rouge2 F1 Score: 0.11813238381054471\n",
    "Average rougeL F1 Score: 0.23140387876731552\n",
    "\n",
    "Looking at the actual results from model generation, for example:\n",
    "```text\n",
    "Context:\n",
    "Doctor: Hi there! I am Doctor Jones, sir.\n",
    "Patient: Hello! It is nice to meet you.\n",
    "Doctor: What brings you into see me today?\n",
    "Patient: I have had this weakness in my right leg for quite some time now.\n",
    "Doctor: How long has this been going on and do you know how you injured yourself?\n",
    "Patient: I think that it was about six months ago that the weakness in my leg started. I don't really remember how it happened.\n",
    "Doctor: Can you tell me what you do remember?\n",
    "Patient: I was reaching to get something from a cabinet, and I noticed that I was unable to stand on my right toe. Ever since then I have had difficulty pushing off when walking. My toes were tingling and numb.\n",
    "Doctor: Was the numbness and tingling mild, moderate, or severe?\n",
    "Patient: It was a mild feeling, but this has been an ongoing problem that has been the same since the weakness started.\n",
    "Doctor: Have you had any other pain any where else in your body?\n",
    "Patient: I have had back pain, but this has been going on for many years and has not changed.\n",
    "Doctor: Is the back pain been mild, moderate, or severe?\n",
    "Patient: I would say mild. I have also been having cramping in both calves.\n",
    "Doctor: How long has that been going on?\n",
    "Patient: For the past year but it stopped about two months ago.\n",
    "\n",
    "Questions:\n",
    "[\"What type of pain are you experiencing in your right leg?\", \"Have you noticed any changes in your right leg since your last visit?\", \"Have you noticed any changes in your right leg since your last visit?\"]\n",
    "\n",
    "Ranked:\n",
    "[3, 2, 1]\n",
    "```\n",
    "The results seem to line up retty close with the test set, and while they may contain different questions thir content and the order in which they could be prioritized is, at least to human evaluation, is done suprisingly well.\n",
    "\n",
    "Model training metrics can be found here: https://wandb.ai/jef1056/CS598/\n",
    "\n",
    "# Plans\n",
    "Additional potential improvments:\n",
    "- Dividing the task better into re-ranking and generation steps\n",
    "- Hyperparam sweek to determine best training metric\n",
    "- Enhance logging and charts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
